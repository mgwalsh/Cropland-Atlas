# GeoSurvey prediction workflows {#geosurvey}

*Edited by: M.G. Walsh, ...? *

Grateful acknowledgement to the team at [**QED**](https://qed.ai/), who built and maintain the GeoSurvey App


## Introduction

Quantifying the geographical extent, location and spatial dynamics of cropland areas, rural and urban settlements and woody vegetation cover provides essential land cover information for monitoring and managing human dominated (*"anthropic"*) landscapes and their socio-economic, health, environmental and ecological impacts. Large portions of Africa remain *"terra incognita"* in this context. This is the main motivation and thematic context for the completely reproducible prediction workflow that is presented here. 

[**GeoSurvey**](https://geosurvey.qed.ai/) is a platform for analyzing geospatial land cover observations. High resolution satellite images, mobile phone photography, and drone imagery can be examined and sytematically labeled by either trained photo interpreters and/or by *"crowds"* of citizen scientists. When done with care, these observations result in large, well-structured, properly labeled, geospatial datasets that are suitable for machine learning and geostatistical predictions of land cover and in some instances for inferring land use. The detailed manual for conducting your own GeoSurveys is available at: [**GeoSurvey manual**](https://docs.google.com/document/d/1y-HYUSYpDVESPdmEcl3I2kuL0bwrT41wMiq0zE9uzOs/edit). The manual should definitely be consulted to obtain information about how GeoSurvey is or can be carried out physically/practically. We shall not cover that in this chapter and start from the perspective that we already have GeoSurvey data in hand.

This chapter provides an overview of some of proposed ensemble machine learning and statistical workflows, which we have used for mapping and interpreting GeoSurvey land cover observations from Digital Globe (Worlview etc) satellite images. We recognize that there are potentially many different techniques and algorithms that could be applied in this context. Hence, the workflows presented here are completely open for criticism and improvement. They could (and probably should) also be exposed and stress-tested in data science competitions e.g., at [**Kaggle**](https://www.kaggle.com/), given their current importance for managing terra incognita landscapes. 

The main intent is to provide some starter code for predictive mapping and statistical small area estimation (SAE) of variables such as cropland area, building densities, settlement occurrences and woody vegetation cover that largely define *anthropic* land cover types in either a given country or in any other large geographical region of interest. We use the most recent GeoSurvey data and gridded covariates from Tanzania to illustrate the main analysis steps. These include sequentially:  

1. Data wrangling
2. Spatial prediction using ensemble machine learning
3. Small area estimation
4. Spatial monitoring and prediction

In order to monitor changing landscapes the first three steps should be repeated over time; i.e., to facilitate step 4. The end of this chapter gives some suggestions on as to how to do just that, though we cannot generate a fully-worked example of space-time land cover predictions at this point in time. 

## Step 1: Data wrangling

Assembling analysis-ready dataframes is typically a major task that can take an inordinate amount of a data scientist's time and effort both inside and outside of the actual prediction workflows (e.g., for covariate image and GIS pre-processing). Data providers are encouraged to provide *"clean"* datasets. While GeoSurvey does the *"clean bits"* pretty much automatically, in some instances the veracity of the observations, particularly from crowd-sourced data collections, should be thoroughly quality checked by GeoSurvey administrators. There is an example script for doing that at: [**Input screening**](https://github.com/mgwalsh/Geosurvey/blob/master/GS_input_screening.R).

We will use the following R packages to assemble a dataframe for Tanzania, as an example of an overall workflow:

```{r}
suppressPackageStartupMessages({
  require(downloader)
  require(rgdal)
  require(jsonlite)
  require(raster)
  require(leaflet)
  require(htmlwidgets)
  require(wordcloud)
})
```


### Load all the data locally

There are already many examples of GeoSurveys of different countries and other regions of interest that are available. For Africa, exemplar, *"tidied-up"* data are made publicly available on the [**Open Science Framework**](https://osf.io/vxc97/) repository, which also mirrors all of the associated data assembly and prediction code at [**Github**](https://github.com/mgwalsh/Cropland-Atlas).

The following chunk loads all of the relevant data for Tanzania from its [**TanSIS OSF repo**](https://osf.io/j8y3z/) & Dropbox:

```{r}
# set working directory
dir.create("TZ_GS18", showWarnings = F)
setwd("./TZ_GS18")

# download GeoSurvey data
# see sampling frame @ https://github.com/mgwalsh/Sampling/blob/master/TZ_GS_sample.R
download("https://www.dropbox.com/s/0x4y4j6ifqidmhh/TZ_geos_2018.csv.zip?raw=1", "TZ_geos_2018.csv.zip", mode = "wb")
unzip("TZ_geos_2018.csv.zip", overwrite = T)
geos <- read.table("TZ_geos_2018.csv", header = T, sep = ",")
geos$BIC <- as.factor(ifelse(geos$CP == "Y" & geos$BP == "Y", "Y", "N")) ## identifies croplands with buildings

# download GADM-L3 shapefile (courtesy: http://www.gadm.org)
download("https://www.dropbox.com/s/bhefsc8u120uqwp/TZA_adm3.zip?raw=1", "TZA_adm3.zip", mode = "wb")
unzip("TZA_adm3.zip", overwrite = T)
shape <- shapefile("TZA_adm3.shp")

# download raster stack (note this is a big 1+ Gb download, which a take a while to download depending ...)
# download("https://www.dropbox.com/s/ejl3h62hojnhh3a/TZ_250m_2019.zip?raw=1", "TZ_250m_2019.zip", mode = "wb")
unzip("TZ_250m_2019.zip", overwrite = T)
glist <- list.files(pattern="tif", full.names = T)
grids <- stack(glist)
grids ## examines the structure of the grid stack and its dimensions
```


### Spatial covariates (grids)

People ask what are all the spatial covariates that are used in the various predictions and where do they come from? While these keep changing these as we get access to new open RS/GIS data. However, they are often long-term averages and/or fairly slow geosptial variables, which are periodically updated as warranted. For Tanzania these currently include:

1. Climate related variables from e.g., [**CHELSA**](http://chelsa-climate.org/downloads/)
    - long-term mean temperature and precipitation 
    - mean annual temperature range
    - mean annual precipitation seasonality
2. Terrain data from e.g., [**MDEM**](http://hydro.iis.u-tokyo.ac.jp/~yamadai/MERIT_DEM/)
    - digital elevation
    - various terrain model derrivatives including slope and compound topographic index.
3. Central place theory based variables from various sources
    - distances to major and minor roads from [**Geofabrick**](#https://www.geofabrik.de/data/download.html)
    - distances to known cities, towns and villages from [**Geofabrick**](#https://www.geofabrik.de/data/download.html)
    - distances to national, parks, wildlife conservation areas and forest reserves [**Protected Planet**](http://www.protectedplanet.net/)
    - distance to cell towers [**OpenCell**](https://unwiredlabs.com)
    - distance to main electrical grid
    - distance to open water sources [**ProtectedPlanet**](http://www.protectedplanet.net/)
4. Moderate resolution sensing data from various sources
    - long-term average Terra & Aqua MODIS data 
    - annual average Sentinel 1 & 2 data [**Copernicus**](http://lcviewer.vito.be/)
    - annual average Landsat-8 data [**USGS**](https://gisgeography.com/usgs-earth-explorer-download-free-landsat-imagery/)
    - long term average night lights [**NASA**](https://earthobservatory.nasa.gov/features/NightLights/page3.php)
5. Previous predictions of soil and land cover models e.g.,
    - ESA land cover fractions [**Copernicus**](http://lcviewer.vito.be/)
    - CIESIN/FaceBook high resolution settlement data [**HRSL**](https://ciesin.columbia.edu/data/hrsl/)
    - ISRIC predictive soil maps [**SoilGrids**](https://soilgrids.org)
    - AfSIS prediction results from previous GeoSurveys [**AfSIS**](https://osf.io/vxc97/)
6. High-resolution remote sensing data where needed and affordable e.g.,
    - from [**Planet Labs**](https://www.planet.com/)
    - from [**MAXAR**](https://www.digitalglobe.com/)
    
The full current listing of the Tanzania grids is available is available in one of the [**Tanzania reports 2019**](https://osf.io/7acs3/). All of the layers were resampled to 250 m resolution on a common Lambert-Azimuthal Equal Area coordinate reference system `CRS(+proj=laea +ellps=WGS84 +lon_0=20 +lat_0=5 +units=m +no_defs")`. Note that the production of the grid stack is generally done outside of R, in [**GRASS**](https://grass.osgeo.org/), because it is much faster and convenient to do it there.
  

### Assemble all of the data {#geosurvey_step1}

The chunk below assembles the GeoSurvey and gridded data. It includes total building counts and cropland proportion estimates based on a grid count (both per 6.25 ha quadrat), and it subsequently combines everything with the gridded variables in a consistent dataframe that can be used for spatial prediction.

```{r}
# attach GADM-L3 admin unit names from shape
coordinates(geos) <- ~lon+lat
projection(geos) <- projection(shape)
gadm <- geos %over% shape
geos <- as.data.frame(geos)
geos <- cbind(gadm[ ,c(5,7,9)], geos)
colnames(geos) <- c("region","district","ward","survey","time","id","observer","lat","lon","BP","CP","WP","rice","bloc","cgrid","BIC")

# Coordinates and number of buildings per quadrat -------------------------
bp <- geos[which(geos$BP == "Y"), ] ## identify quadrats with buildings
bp$bloc <- as.character(bp$bloc)

# coordinates of tagged building locations from quadrats with buildings
c <- fromJSON(bp$bloc[1])
bcoord <- do.call("rbind", c$feature$geometry$coordinates)
for(i in 2:nrow(bp)) {
  c <- fromJSON(bp$bloc[i])
  bcoord_temp <- do.call("rbind", c$feature$geometry$coordinates)
  bcoord <- rbind(bcoord, bcoord_temp)
}
bcoord <- as.data.frame(bcoord) ## vector of coordinates per quadrats with buildings
colnames(bcoord) <- c("lon","lat")

# number of tagged building locations from quadrats with buildings
bcount <- rep(NA, nrow(bp))
for(i in 1:nrow(bp)) {
  t <- fromJSON(bp$bloc[i])
  bcount[i] <- nrow(t$features)
}
ba <- geos[which(geos$BP == "N"), ]
ba$bcount <- 0
bp <- cbind(bp, bcount)
geos <- rbind(ba, bp)
geos <- geos[order(geos$id),] ## sort in original sample order

# cropland grid count
cp <- geos[which(geos$CP == "Y"), ] ## identify quadrats with cropland
cp$cgrid <- as.character(cp$cgrid)

# number of tagged grid locations from quadrats with cropland
ccount <- rep(NA, nrow(cp))
for(i in 1:nrow(cp)) {
  t <- fromJSON(cp$cgrid[i])
  ccount[i] <- nrow(t$features)
}
ca <- geos[which(geos$CP == "N"), ]
ca$ccount <- 0
cp <- cbind(cp, ccount)
geos <- rbind(ca, cp)
geos <- geos[order(geos$id),] ## sort in original sample order

# project GeoSurvey coords to grid CRS
geos.proj <- as.data.frame(project(cbind(geos$lon, geos$lat), "+proj=laea +ellps=WGS84 +lon_0=20 +lat_0=5 +units=m +no_defs"))
colnames(geos.proj) <- c("x","y")
geos <- cbind(geos, geos.proj)
coordinates(geos) <- ~x+y
projection(geos) <- projection(grids)

# extract gridded variables at GeoSurvey locations
geosgrid <- extract(grids, geos)
gsdat <- as.data.frame(cbind(geos, geosgrid)) 
# gsdat <- gsdat[!duplicated(gsdat), ] ## removes any duplicates ... if needed
gsdat <- gsdat[complete.cases(gsdat[ ,c(10:13,19:67)]),] ## removes incomplete cases
gsdat$observer <- sub("@.*", "", as.character(gsdat$observer)) ## shortens observer ID's
```


### GeoSurvey dataframe 

The main end product of all of this wrangling is a dataframe called `gsdat` which contains the various GeoSurvey observations as well the gridded observations and corresponding administrative boundaries from [**GADM**](https://gadm.org/data.html). You can look at the overall structure of this file with:

```{r}
str(gsdat)
```

Note that there are n = 16,219 (6.25 ha) quadrats (rows) and 69 variables (columns) in this dataframe. The actual GeoSurvey variables that we'd like to predict from the spatial covariates in this particular case include:

1. **BP**, the observed presence/absence of buildings in a quadrat
2. **CP**, the observed presence/absence of croplands in a quadrat
3. **WP**, the observed presence/absence of woody vegetation cover > 60% in a quadrat
4. **rice**, the presence/absence of paddy rice in a quadrat
5. **bcount**, a count of all tagged buildings in a quadrat
6. **ccount**, a grid count of the proportion of cropland area (n/16) in a quadrat

BP, CP, and WP predictions are used to generate a Land Cover Classification (LCC) focused on anthropic parts of landscapes, which we use for survey post-stratification, small area estimation and development of ground-sampling plans (e.g, for infrastructure, soil and cropping systems sampling plans that involve sending teams into the field).

The following chunk writes the `gsdat` file and others out that are subsequently used for various spatial predictions. Writing the relevant files out simply means that one does not have to run the data wrangling step repeatedly if the underlying data have not changed. It also produces simple Leaflet maps of sampling locations and/or tagged building coordinates (see below).

```{r}
dir.create("Results", showWarnings = F)
write.csv(bcoord, "./Results/TZ_bcoord.csv", row.names = F)
write.csv(gsdat, "./Results/TZ_gsdat_2018.csv", row.names = F)

# GeoSurvey map widget ----------------------------------------------------
w <- leaflet() %>%
  setView(lng = mean(gsdat$lon), lat = mean(gsdat$lat), zoom = 6) %>%
  addProviderTiles(providers$OpenStreetMap.Mapnik) %>%
  addCircleMarkers(gsdat$lon, gsdat$lat, clusterOptions = markerClusterOptions())
saveWidget(w, 'TZ_GS18.html', selfcontained = T) ## save widget
w ## plot widget 
```
**Figure 1**: Distribution of GeoSurvey locations completed in Tanzania (2018).

Individual GeoSurvey contributions are acknowledged with a simple wordcloud, which highlights the proportion of the quadrats that were completed by each GeoSurveyor. This essentially provides the survey signature for quality control purposes.

```{r}
# GeoSurvey contributions -------------------------------------------------
gscon <- as.data.frame(table(gsdat$observer))
set.seed(1235813)
wordcloud(gscon$Var1, freq = gscon$Freq, scale = c(4,0.1), random.order = T)
```
**Figure 2**: Tanzania GeoSurvey contributions (2018).


## Step 2: Spatial prediction using ensemble machine learning {#geosurvey_step2}

We now turn to the prediction part of the workflow. We acknowledge that everyone appears to have their favorite algorithm(s) for predicting the types geospatial data that GeoSurvey produces. The notion of there being favorite algorithms supports our philosophy that we might be able use some sort of a combination of those (favorites) to derive a weighted (*"ensemble"*) prediction at the end of the process. Our basic schema for this is shown in the Figure below: ...


